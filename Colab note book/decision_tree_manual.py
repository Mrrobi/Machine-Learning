# -*- coding: utf-8 -*-
"""Decision Tree Manual.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17P4wuuiEBe9ew4jHnfeh4aNe0LXF-Evp
"""

import numpy as np
import math
import random
import pandas as pd
from sklearn.model_selection import train_test_split
from collections import Counter
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef

"""# Decision tree"""

class Node:
    def __init__(self, attribute=None, attribute_values=None, child_nodes=None, decision=None):
        self.attribute = attribute
        self.attribute_values = attribute_values
        self.child_nodes = child_nodes
        self.decision = decision


class DecisionTree:

    root = None

    @staticmethod
    def plurality_values(data):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        unique,pos = np.unique(labels,return_inverse=True) #Finds all unique elements and their positions
        counts = np.bincount(pos)                     #Count the number of each unique element
        maxpos = counts.argmax()  
        return unique[maxpos]


    @staticmethod
    def all_zero(data):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        flag = True
        for i in labels:
          if(i != 0):
            flag = False
            return flag
        return flag

    @staticmethod
    def all_one(data):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        flag = True
        for i in labels:
          if(i != 1):
            flag = False
            return flag
        return flag

    @staticmethod
    def B(p,n):
      import math
      B = (p/(p+n))
      if(B==0):
        return - (1)*math.log2(1)
      elif(B==1):
        return -B*math.log2(B)
      return -B*math.log2(B) - (1-B)*math.log2(1-B)

    @staticmethod
    def importance(data, attributes):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        count_label = Counter(labels)
        p = count_label[1]
        n = count_label[0]
        paren_ent = DecisionTree.B(p,n)
        max_gain = -100000
        max_gain_col = 0
        for col in attributes:
          sample_count = Counter(data[:, col])
          unik_values = np.unique(data[:, col])
          total = 0
          for k in range(len(unik_values)):
            pk, nk = DecisionTree.guntesi(data,col,unik_values[k])
            total += ((pk+nk)/(p+n))*DecisionTree.B(pk,nk)
          gain = paren_ent - total
          if(gain>=max_gain):
            max_gain = gain
            max_gain_col = col
        return max_gain_col



    def train(self, data, attributes, parent_data):
        data = np.array(data)
        parent_data = np.array(parent_data)
        attributes = list(attributes)

        if data.shape[0] == 0:  # if x is empty
            return Node(decision=self.plurality_values(parent_data))

        elif self.all_zero(data):
            return Node(decision=0)

        elif self.all_one(data):
            return Node(decision=1)

        elif len(attributes) == 0:
            return Node(decision=self.plurality_values(data))

        else:
            a = self.importance(data, attributes)
            tree = Node(attribute=a, attribute_values=np.unique(data[:, a]), child_nodes=[])
            attributes.remove(a)
            for vk in np.unique(data[:, a]):
                new_data = data[data[:, a] == vk, :]
                subtree = self.train(new_data, attributes, data)
                tree.child_nodes.append(subtree)

            return tree
    @staticmethod
    def guntesi(df,col,sample):
        won = 0
        nowin = 0
        for i in range(len(df)):
          if(df[i][col]==sample):
            if(df[i][9]==1):
              won+=1
            else:
              nowin+=1
        return won,nowin

    def fit(self, data):
        self.root = self.train(data, list(range(data.shape[1] - 1)), np.array([]))

    def predict(self, data):
        predictions = []
        for i in range(data.shape[0]):
            current_node = self.root
            while True:
                if current_node.decision is None:
                    current_attribute = current_node.attribute
                    current_attribute_value = data[i, current_attribute]
                    if current_attribute_value not in current_node.attribute_values:
                        predictions.append(random.randint(0, 1))
                        break
                    idx = list(current_node.attribute_values).index(current_attribute_value)

                    current_node = current_node.child_nodes[idx]
                else:
                    predictions.append(current_node.decision)
                    break

        return predictions

"""# New Section"""

import pandas as pd
import numpy as np

data = pd.read_csv("/content/tic-tac-toe.csv")
data

x = data.iloc[:, :data.shape[1] - 1]
y = data.iloc[:, data.shape[1] - 1]

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20)
x_train = x_train.reset_index(drop=True)
x_test = x_test.reset_index(drop=True)
print(y_train.shape)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
encoder.fit(y_train)
y_train = encoder.transform(y_train)
y_true = encoder.transform(y_test)
y_train = pd.Series(y_train)
y_train

x_train['class']=y_train

train = x_train.to_numpy()
clf = DecisionTree()
clf.fit(train)

x_test = x_test.to_numpy()

y_pred = clf.predict(x_test)

acc = accuracy_score(y_true,y_pred)
pre = precision_score(y_true,y_pred)
re = recall_score(y_true,y_pred)
f1 = f1_score(y_true,y_pred)
print('Accuracy: ',acc)
print('Precision: ',pre)
print('Recall: ',re)
print('f1-Score: ',f1)